# -*- coding: utf-8 -*-
"""Text_generation_with_GPT-2_PRODIGY_GA_01

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nGdye1UdhGXmT-dUz2kncxMaLzdAogV3

# Install and Import
"""

!pip install -q gradio
!pip install -q git+https://github.com/huggingface/transformers.git

import gradio as gr
import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

!pip install --upgrade gradio

"""# Function to Update and Use Memory"""

def update_memory(key, value):
    memory[key] = value
    return f"Momory updated: {key} is now {value}"

def generate_text(inp):
    input_ids = tokenizer.encode(inp, return_tensors='tf')
    beam_output = model.generate (input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)
    Output = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return ".".join(Output.split(".")[:-1]) + "."

"""# Integrated with Gradio Interface"""

from transformers import GPT2Tokenizer,TFGPT2LMHeadModel

# Load tokenizer and model
tokenizer=GPT2Tokenizer.from_pretrained("gpt2")
model=TFGPT2LMHeadModel.from_pretrained("gpt2",pad_token_id=tokenizer.eos_token_id)

# Define functions
def update_memory(key,value):
  memory[key]=value
  return f"Memory updated: {key} is now {value}"

# Create gradio interface
input_textbox = gr.Textbox(lines=2,placeholder="Enter a sentence...")
memory_key_textbox= gr.Textbox(lines=1,placeholder="Memory key...")
memory_value_textbox=gr.Textbox(lines=1,placeholder="Memory value...")

generate_btn=gr.Button("Generate Text")
update_memory_btn=gr.Button("Update Memory")

generate_text_interface=gr.Interface(fn=generate_text,inputs=input_textbox,outputs="text",title="GPT-2 Text Generation")

# Launch the interface
gr.TabbedInterface([generate_text_interface], tab_names = ["Generate Text"]).launch()